{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "CS 4780 Final Project Student Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUITzFlDJcZ5"
      },
      "source": [
        "<h2>Election Result Prediction for US Counties</h2>\n",
        "\n",
        "Vaed Prasad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52js1-KJcZ5"
      },
      "source": [
        "<h3>Introduction:</h3>\n",
        "\n",
        "\n",
        "Economic and sociological factors have been widely used when making predictions on the voting results of US elections. Economic and sociological factors vary a lot among counties in the United States. In addition, as observed from the election map of recent elections, neighbor counties show similar patterns in terms of the voting results. In this project we will bring the power of machine learning to make predictions for the county-level election results using Economic and sociological factors and the geographic structure of US counties. </p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DwIOs6OJcZ6"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu8tsNm3JcZ6"
      },
      "source": [
        "<h3>1.2 Weighted Accuracy:</h3><p>\n",
        "Since our dataset labels are heavily biased, we will use the following function to compute weighted accuracy throughout our training and validation process.\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMw87_0LJcZ6"
      },
      "source": [
        "def weighted_accuracy(pred, true):\n",
        "    assert(len(pred) == len(true))\n",
        "    num_labels = len(true)\n",
        "    num_pos = sum(true)\n",
        "    num_neg = num_labels - num_pos\n",
        "    frac_pos = num_pos/num_labels\n",
        "    weight_pos = 1/frac_pos\n",
        "    weight_neg = 1/(1-frac_pos)\n",
        "    num_pos_correct = 0\n",
        "    num_neg_correct = 0\n",
        "    for pred_i, true_i in zip(pred, true):\n",
        "        num_pos_correct += (pred_i == true_i and true_i == 1)\n",
        "        num_neg_correct += (pred_i == true_i and true_i == 0)\n",
        "    weighted_accuracy = ((weight_pos * num_pos_correct) \n",
        "                         + (weight_neg * num_neg_correct))/((weight_pos * num_pos) + (weight_neg * num_neg))\n",
        "    return weighted_accuracy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l92LmB3MJcZ6"
      },
      "source": [
        "<h2>Part 2: Baseline Solution</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG1Jd0vyJcZ6"
      },
      "source": [
        "<h3>2.1 Preprocessing and Feature Extraction:</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd9gQMINJcZ7"
      },
      "source": [
        "# Creates a dataframe from the csv files\n",
        "# to run the code, all relavant files must be placed in a folder named 'data'\n",
        "file_dir = 'data/'\n",
        "test = 'test_2016_no_label.csv'\n",
        "train = 'train_2016.csv'\n",
        "graph = 'graph.csv'\n",
        "train_2012 = 'train_2012.csv'\n",
        "test_2012 = 'test_2012_no_label.csv'\n",
        "\n",
        "train_df = pd.read_csv(file_dir + train)\n",
        "test_df = pd.read_csv(file_dir + test)\n",
        "\n",
        "neighboring_counties = pd.read_csv(file_dir + graph)\n",
        "\n",
        "df_2012_test = pd.read_csv(file_dir + test_2012)\n",
        "df_2012_train = pd.read_csv(file_dir + train_2012)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leZCCMM37WuN"
      },
      "source": [
        "# Normalizes the numpy array by the given mean and standard deviation\n",
        "def normalize_by_param(X, mean, std):\n",
        "  \"\"\"Return normalized dataset using given mean and standard deviation\"\"\"\n",
        "  X = np.divide(np.subtract(X, mean), std)\n",
        "  return X\n",
        "\n",
        "# Normalizes the numpy array, returning also the mean and standard deviation\n",
        "def normalize(X):\n",
        "  \"\"\"Return normalized dataset, mean, and standard deviation\"\"\"\n",
        "  mean = np.mean(X, axis=0)\n",
        "  std = np.std(X, axis=0)\n",
        "  X = normalize_by_param(X, mean, std)\n",
        "  return X, mean, std\n",
        "\n",
        "# Normalizes the dataframe, returning also the mean and standard deviation\n",
        "def normalize_df(df):\n",
        "  \"\"\"Return normalized dataframe, mean, and standard deviation\"\"\"\n",
        "  mean = df.mean()\n",
        "  std = df.std()\n",
        "  return normalize_df_by_param(df, mean, std), mean, std\n",
        "\n",
        "# Normalizes the dataframe by the given mean and standard deviation\n",
        "def normalize_df_by_param(df, mean, std):\n",
        "  \"\"\"Return normalized dataframe using given mean and standard deviation\"\"\"\n",
        "  return (df - mean) / std\n",
        "\n",
        "# Converts the 'MedianIncome' field in the dataframe to float from string\n",
        "def parse_income(df):\n",
        "  \"\"\"Return parsed dataframe for 'MedianIncome' by removing commas and converting to type float\"\"\"\n",
        "  df[\"MedianIncome\"] = df[\"MedianIncome\"].replace(',','', regex=True).astype('float')\n",
        "  return df\n",
        "\n",
        "# Returns a numpy array of the labels\n",
        "def get_labels(train_df):\n",
        "  \"\"\"Return dataframe with binary 'winner' feature for county's winner\"\"\"\n",
        "  train_df[\"winner\"] = (train_df[\"DEM\"] > train_df[\"GOP\"]) + 0\n",
        "  return train_df[\"winner\"].to_numpy()\n",
        "\n",
        "# Returns the state code of a county based on its fips code\n",
        "def get_state_code(fips):\n",
        "  \"\"\"Return first two digits of fips code, representing the state code\"\"\"\n",
        "  if len(str(fips)) == 4:\n",
        "    return \"0\" + str(fips)[0:1]\n",
        "  else:\n",
        "    return str(fips)[0:2]\n",
        "\n",
        "# Returns the set of the fips code of all pf a counties neighbors\n",
        "def get_neighbors(fips):\n",
        "  \"\"\"Return set of neighboring counties for county with [fips]\"\"\"\n",
        "  fipses = set()\n",
        "  for _, row in neighboring_counties[neighboring_counties[\"SRC\"] == fips].iterrows():\n",
        "    if (fips != row[\"DST\"]):\n",
        "      fipses.add(row[\"DST\"])\n",
        "\n",
        "  return fipses\n",
        "\n",
        "# Return the class weights based on the proportions of labelings\n",
        "def get_class_weights(Y):\n",
        "  \"\"\"Return proportion of values for binary class\"\"\"\n",
        "  n = Y.shape[0]\n",
        "  return {1: (n - Y.sum()) / n, 0: Y.sum() / n}\n",
        "\n",
        "# Indices of instance variables in the dataframe\n",
        "indices = [\"MedianIncome\",\"MigraRate\",\"BirthRate\", \"DeathRate\", \"BachelorRate\", \"UnemploymentRate\"]\n",
        "neighboring_indices = [\"*MedianIncome\",\"*MigraRate\",\"*BirthRate\", \"*DeathRate\", \"*BachelorRate\", \"*UnemploymentRate\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7T-FlFSTtRb"
      },
      "source": [
        "# Our Basic training set involves preproccessing the test and training dataframes\n",
        "# imported from the csv files by converting the income values into a float data\n",
        "# type and normalizing the data\n",
        "\n",
        "def generateTestandTrain(train_df, test_df):\n",
        "  '''\n",
        "  Parameters: train_df and test_df are the dataframes corresponding to the\n",
        "  training and testing data\n",
        "  Returns: X, xTe, and Y where X and xTe are the normalized and preprocessed\n",
        "  Training and testing set. Y is the set of labeks for training set X \n",
        "  '''\n",
        "  \n",
        "  train_df = parse_income(train_df)\n",
        "  X = train_df[indices].to_numpy()\n",
        "  X, mean, std = normalize(X)\n",
        "  Y = get_labels(train_df)\n",
        "  \n",
        "  test_df = parse_income(test_df)\n",
        "  xTe = test_df[indices].to_numpy()\n",
        "  xTe = normalize_by_param(xTe, mean, std)\n",
        "  \n",
        "  return X, xTe, Y\n",
        "\n",
        "X_Basic, xTe_Basic, Y = generateTestandTrain(train_df, test_df)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "<h3>2.2.1 K - Nearest Neighbors:</h3><p>"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "S2unvGt_JcZ7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jthzt7ZyWHf9"
      },
      "source": [
        "def knn(X,Y):\n",
        "  '''\n",
        "  Parameters: X is the training data set of datatype 2D np.array\n",
        "  and Y is the corresponding labels of type 1D np.array\n",
        "  Returns: Runs K-Fold Validation on the training set run on the k nearest neighbor\n",
        "  model and prints the highest validation accuracy. \n",
        "  Returns the knn model trained on the set with the highest validation accuracy.\n",
        "  '''\n",
        "  kf = KFold(n_splits=10)\n",
        "  kf.get_n_splits(X)\n",
        "  neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "  splits = kf.split(X)\n",
        "\n",
        "  accuracies = []\n",
        "  indices = []\n",
        "  for train_index, valid_index in splits:\n",
        "    neigh.fit(X[train_index], Y[train_index])\n",
        "    acc = weighted_accuracy(neigh.predict(X[valid_index]), Y[valid_index])\n",
        "    accuracies.append(acc)\n",
        "    indices.append(train_index)\n",
        "\n",
        "  best_index = indices[np.argmax(accuracies)]\n",
        "  neigh.fit(X[best_index], Y[best_index])\n",
        "  print(max(accuracies))\n",
        "  return neigh\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "source": [
        "<h3>2.2.2 Support Vector Machine:</h3><p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47z-hMy-W5QO"
      },
      "source": [
        "def svc(X, Y):\n",
        "  '''\n",
        "  Parameters: X is the training data set of datatype 2D np.array\n",
        "  and Y is the corresponding labels of type 1D np.array\n",
        "  Returns: Runs K-Fold Validation on the training set run on an kernelized SVM\n",
        "  model with weighted classes. Returns the SVM model trained on the set with\n",
        "  the highest validation accuracy.\n",
        "  '''\n",
        "\n",
        "  svc = SVC(kernel='rbf', gamma='auto', class_weight=get_class_weights(Y))\n",
        "  kf = KFold(n_splits=10)\n",
        "  kf.get_n_splits(X)\n",
        "\n",
        "  accuracies = []\n",
        "  indices = []\n",
        "  for train_index, valid_index in kf.split(X):\n",
        "    svc.fit(X[train_index], Y[train_index])\n",
        "    acc = weighted_accuracy(svc.predict(X[valid_index]), Y[valid_index])\n",
        "    accuracies.append(acc)\n",
        "    indices.append(train_index)\n",
        "\n",
        "  best_index = indices[np.argmax(accuracies)]\n",
        "  svc.fit(X[best_index], Y[best_index])\n",
        "  print(max(accuracies))\n",
        "  return svc\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "<h3>2.2.3 Neural Network:</h3><p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSGprq4Dnkku"
      },
      "source": [
        "def neural_network(X,Y):\r\n",
        "  '''\r\n",
        "  Parameters: X is the training data set of datatype 2D np.array\r\n",
        "  and Y is the corresponding labels of type 1D np.array\r\n",
        "  Returns: Runs K-Fold Validation on the training set run on the neural network\r\n",
        "  model and prints the highest validation accuracy. \r\n",
        "  Returns the neural network model trained on the set with the highest \r\n",
        "  validation accuracy.\r\n",
        "  '''\r\n",
        "\r\n",
        "  kf = KFold(n_splits=10)\r\n",
        "  kf.get_n_splits(X)\r\n",
        "  nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 10),random_state=1, max_iter = 400, activation = 'tanh')\r\n",
        "  splits = kf.split(X)\r\n",
        "\r\n",
        "  accuracies = []\r\n",
        "  indices = []\r\n",
        "  for train_index, valid_index in splits:\r\n",
        "    nn.fit(X[train_index], Y[train_index])\r\n",
        "    acc = weighted_accuracy(nn.predict(X[valid_index]), Y[valid_index])\r\n",
        "    accuracies.append(acc)\r\n",
        "    indices.append(train_index)\r\n",
        "  \r\n",
        "  print(max(accuracies))\r\n",
        "  best_index = indices[np.argmax(accuracies)]\r\n",
        "  nn.fit(X[best_index], Y[best_index])\r\n",
        "\r\n",
        "  return nn\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v630LSTBJcZ7"
      },
      "source": [
        "<h3>2.3 Training, Validation and Model Selection:</h3><p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-D2bUlQJcZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d957d8a-a38a-4f6c-d0bd-47156aae6061"
      },
      "source": [
        "'''\n",
        "Our model functions (knn, svc, and nn) run a K-Fold validation on the training\n",
        "set and returns the model that was trained on the split that produce the highest\n",
        "accuracies. Here, we print out the accuracies of each of these models and choose\n",
        "the model with the highest accuracy. The accuracy of each model is commented on \n",
        "the side.\n",
        "\n",
        "''' \n",
        "print(\"------------------BASIC---------------------\")\n",
        "knn_basic = knn(X_Basic, Y) #0.7713333333333333\n",
        "svc_basic = svc(X_Basic, Y) #0.8647342995169083\n",
        "nn_basic = neural_network(X_Basic, Y) #0.8115384615384615\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------BASIC---------------------\n",
            "0.7713333333333333\n",
            "0.8647342995169083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8115384615384615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh5XZ0jHJcZ7"
      },
      "source": [
        "<h3>2.4 Initial Results:</h3><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qedg8Y9QJcZ7"
      },
      "source": [
        "\n",
        "2.4.1 *How did you preprocess the dataset and features?*\n",
        "\n",
        "In order to clean the dataset and features, we first parsed the “MedianIncome” by removing commas and converting the data type from a string to a float. Next, we calculated each column’s mean and standard deviation in order to normalize the training data frame by subtracting the mean and dividing by the standard deviation for each entry in a given column. We also created a binary “winner” feature column by comparing the number of “GOP” votes and “DEM” votes in a given county, where 1 represents that the Democrats won the county and 0 represents that the Republicans won the county. Finally, we also normalized our test set with the training set’s mean and standard deviation.\n",
        "\n",
        "2.4.2 *Which two learning methods from class did you choose and why did you made the choices?*\n",
        "\n",
        "The two learning methods we chose from class was K-Nearest Neighbors and Soft-Margin Support Vector Machine. We selected K-Nearest Neighbors because we believed that counties with similar political voting patterns will be clustered near each other in the feature space. Similarly, we selected a Soft-Margin Support Vector Machine because we believed that the features would separate Republican counties from Democratic counties far enough to build an accurate support vector machine.\n",
        "\n",
        "2.4.3 *How did you do the model selection?*\n",
        "\n",
        "For both our K-Nearest Neighbors and Soft-Margin Support Vector Machine learning methods we leveraged k-fold validation by making 10 splits and selecting the trained model with the best accuracy. Then, we compared the accuracy of the best performing K-Nearest Neighbors model and Soft-Margin Support Vector Machine model and selected the SVM model as it yielded a better accuracy. Our model had a test performance of 70.36%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wcGZVizJcZ7"
      },
      "source": [
        "<h2>Part 3: Feature Adjustments</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_ijCqoJcZ7"
      },
      "source": [
        "<h3>3.1.1 Considering 2012 Election Data:</h3><p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imQIcCfPnfxu"
      },
      "source": [
        "\r\n",
        "# Our 2012 training set involves preproccessing the test and training dataframes\r\n",
        "# imported from the csv files by converting the income values into a float data\r\n",
        "# type and normalizing the data. We also take it one step further than the basic\r\n",
        "# training set by adding in new features that represent the change in features\r\n",
        "# from 2012 to 2016. The feature changes that we looked at were change in \r\n",
        "# Migration Rate, Birth Rate, Death Rate, Unemployment Rate, and Median Income\r\n",
        "\r\n",
        "def generateTestandTrain2012(train_creative, test_creative, df_2012_train, df_2012_test):\r\n",
        "  \r\n",
        "  '''\r\n",
        "  Parameters: train_creative and test_creative are the dataframes corresponding \r\n",
        "  to the 2016 training and testing data; df_2012_train and df_2012_test are the \r\n",
        "  dataframes corresponding to the 2012 training and testing data\r\n",
        "  Returns: X_Creative, xTe_Creative, Y where X_Creative andxTe_Creative are the \r\n",
        "  normalized and preprocessed training and testing set. Y is the set of labels \r\n",
        "  for training set X_Creative\r\n",
        "  '''\r\n",
        "\r\n",
        "  df_2012_train = parse_income(df_2012_train)\r\n",
        "  train_creative = parse_income(train_creative)\r\n",
        "  train_creative[\"MigraRateChange\"] = df_2012_train[\"MigraRate\"] - train_creative[\"MigraRate\"]\r\n",
        "  train_creative[\"BirthRateChange\"] = df_2012_train[\"BirthRate\"] - train_creative[\"BirthRate\"]\r\n",
        "  train_creative[\"DeathRateChange\"] = df_2012_train[\"DeathRate\"] - train_creative[\"DeathRate\"]\r\n",
        "  train_creative[\"UnemploymentChange\"] = df_2012_train[\"UnemploymentRate\"] - train_creative[\"UnemploymentRate\"]\r\n",
        "  train_creative[\"MedianIncomeChange\"] = df_2012_train[\"MedianIncome\"] - train_creative[\"MedianIncome\"]\r\n",
        "\r\n",
        "  train_creative = parse_income(train_creative)\r\n",
        "\r\n",
        "  X_Creative = train_creative[indices + [\"MigraRateChange\",\"BirthRateChange\", \"DeathRate\", \"UnemploymentChange\",\"MedianIncomeChange\"]].to_numpy()\r\n",
        "  X_Creative, mean_creative, std_creative = normalize(X_Creative)\r\n",
        "\r\n",
        "  df_2012_test = parse_income(df_2012_test)\r\n",
        "  test_creative = parse_income(test_creative)\r\n",
        "\r\n",
        "  test_creative[\"MigraRateChange\"] = df_2012_test[\"MigraRate\"] - test_creative[\"MigraRate\"]\r\n",
        "  test_creative[\"BirthRateChange\"] = df_2012_test[\"BirthRate\"] - test_creative[\"BirthRate\"]\r\n",
        "  test_creative[\"DeathRateChange\"] = df_2012_test[\"DeathRate\"] - test_creative[\"DeathRate\"]\r\n",
        "  test_creative[\"UnemploymentChange\"] = df_2012_test[\"UnemploymentRate\"] - test_creative[\"UnemploymentRate\"]\r\n",
        "  test_creative[\"MedianIncomeChange\"] = df_2012_test[\"MedianIncome\"] - test_creative[\"MedianIncome\"]\r\n",
        "\r\n",
        "  xTe_Creative = test_creative[indices + [\"MigraRateChange\",\"BirthRateChange\", \"DeathRate\", \"UnemploymentChange\",\"MedianIncomeChange\"]].to_numpy()\r\n",
        "  xTe_Creative = normalize_by_param(xTe_Creative, mean_creative, std_creative)\r\n",
        "\r\n",
        "  Y = get_labels(train_df)\r\n",
        "\r\n",
        "  return X_Creative, xTe_Creative, Y\r\n",
        "\r\n",
        "X_2012, xTe_2012, Y = generateTestandTrain2012(train_df, test_df, df_2012_train, df_2012_test)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "source": [
        "<h3>3.1.2 Considering Neighboring Counties:</h3><p>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpUqQ-nCnra_"
      },
      "source": [
        "# Our neighbors training set involves preproccessing the test and training dataframes\r\n",
        "# imported from the csv files by converting the income values into a float data\r\n",
        "# type and normalizing the data. We also take it one step further than the basic\r\n",
        "# training set by adding in a new feature that represents the number of neighbors \r\n",
        "# a county has. We too this data from the graph.csv file. \r\n",
        "\r\n",
        "def generateTestandTrainneighbors(train_creative, test_creative, neighboring_counties):\r\n",
        "  '''\r\n",
        "  Parameters: train_creative and test_creative are the dataframes corresponding \r\n",
        "  to the 2016 training and testing data; neighboring_counties is the dataframe\r\n",
        "  corresponding to the number of neighboring counties each county has\r\n",
        "  Returns: X_Creative, xTe_Creative, Y where X_Creative andxTe_Creative are the \r\n",
        "  normalized and preprocessed training and testing set. Y is the set of labels \r\n",
        "  for training set X_Creative\r\n",
        "  '''\r\n",
        "\r\n",
        "  #creates a dictionary that counts how many times a FIPS number occurs in the \r\n",
        "  #neighboring_counties dataframe \r\n",
        "\r\n",
        "  train_creative[\"neighbors\"] = 0\r\n",
        "  number_neighbors = []\r\n",
        "  frequencies = neighboring_counties[\"SRC\"].value_counts()\r\n",
        "  for i in train_creative[\"FIPS\"]:\r\n",
        "    number_neighbors.append(frequencies[i])\r\n",
        "\r\n",
        "  # adds this new feature to our training set\r\n",
        "  train_creative[\"neighbors\"] = number_neighbors\r\n",
        "  train_creative = parse_income(train_creative)\r\n",
        "  X_creative = train_creative[indices].to_numpy()\r\n",
        "  X_creative, mean, std = normalize(X_creative)\r\n",
        "  \r\n",
        "  test_creative[\"neighbors\"] = 0\r\n",
        "  number_neighbors = []\r\n",
        "\r\n",
        "  for i in test_creative[\"FIPS\"]:\r\n",
        "    number_neighbors.append(frequencies[i])\r\n",
        "\r\n",
        "  # adds this new feature to our testing set\r\n",
        "  test_creative[\"neighbors\"] = number_neighbors\r\n",
        "  test_creative = parse_income(test_creative)\r\n",
        "  xTeCreative = test_creative[indices].to_numpy()\r\n",
        "  xTeCreative = normalize_by_param(xTeCreative, mean, std)\r\n",
        "\r\n",
        "  Y = get_labels(train_creative)\r\n",
        "\r\n",
        "  return X_creative, xTeCreative, Y\r\n",
        "\r\n",
        "X_neighbors, xTe_neighbors, Y = generateTestandTrainneighbors(train_df, test_df, neighboring_counties)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJDvanFDn3OC"
      },
      "source": [
        "  # Our neighbors training set involves preproccessing the test and training dataframes\r\n",
        "  # imported from the csv files by converting the income values into a float data\r\n",
        "  # type and normalizing the data. We also take it one step futher and add a binary\r\n",
        "  # instance variable for every single state, where a value of 1 would indicate\r\n",
        "  # that a certain county is located in that state \r\n",
        "  def generateTestAndTrainStates(train_creative, test_creative):\r\n",
        "    '''\r\n",
        "    Parameters: train_df and test_df are the dataframes corresponding to the\r\n",
        "    training and testing data\r\n",
        "    Returns: X, xTe, and Y where X and xTe are the normalized and preprocessed\r\n",
        "    Training and testing set. Y is the set of labeks for training set X \r\n",
        "    '''\r\n",
        "\r\n",
        "    states = []\r\n",
        "\r\n",
        "    train_creative = parse_income(train_creative)\r\n",
        "    train_creative[indices], mean, std = normalize_df(train_creative[indices])\r\n",
        "\r\n",
        "    test_creative = parse_income(test_creative)\r\n",
        "    test_creative[indices] = normalize_df_by_param(test_creative[indices], mean, std)\r\n",
        "\r\n",
        "    states = []\r\n",
        "    for _, row in train_creative.iterrows():\r\n",
        "      s = get_state_code(row[\"FIPS\"])\r\n",
        "      if (s not in states):\r\n",
        "        states.append(s)\r\n",
        "\r\n",
        "    train_creative[states] = 0\r\n",
        "    test_creative[states] = 0\r\n",
        "\r\n",
        "    for i, row in train_creative.iterrows():\r\n",
        "      s = get_state_code(row[\"FIPS\"])\r\n",
        "      train_creative.loc[i, s] = 1\r\n",
        "    for i, row in test_creative.iterrows():\r\n",
        "      s = get_state_code(row[\"FIPS\"])\r\n",
        "      test_creative.loc[i, s] = 1\r\n",
        "\r\n",
        "    X_creative = train_creative[indices + states].to_numpy()\r\n",
        "    xTeCreative = test_creative[indices + states].to_numpy()\r\n",
        "\r\n",
        "    Y = get_labels(train_creative)\r\n",
        "\r\n",
        "    return X_creative, xTeCreative, Y\r\n",
        "\r\n",
        "X_states, xTe_states, Y = generateTestAndTrainStates(train_df, test_df)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO9TTWO_nxfx"
      },
      "source": [
        "  # Our neighbors training set involves preproccessing the test and training dataframes\r\n",
        "  # imported from the csv files by converting the income values into a float data\r\n",
        "  # type and normalizing the data. We also take it one step futher and add a duplicate\r\n",
        "  # instance variable for all the once used in our basic training data, which we assign\r\n",
        "  # to be a weighted average of all of the nearby counties (in this case, all counties that\r\n",
        "  # are less than 2 border crossings away from the original)\r\n",
        "  def generateTestAndTrainNeighborsAverage(train_creative, test_creative):\r\n",
        "    '''\r\n",
        "    Parameters: train_df and test_df are the dataframes corresponding to the\r\n",
        "    training and testing data\r\n",
        "    Returns: X, xTe, and Y where X and xTe are the normalized and preprocessed\r\n",
        "    Training and testing set. Y is the set of labeks for training set X \r\n",
        "    '''\r\n",
        "\r\n",
        "    train_creative = parse_income(train_creative)\r\n",
        "    train_creative[indices], mean, std = normalize_df(train_creative[indices])\r\n",
        "\r\n",
        "    test_creative = parse_income(test_creative)\r\n",
        "    test_creative[indices] = normalize_df_by_param(test_creative[indices], mean, std)\r\n",
        "\r\n",
        "    combined_creative = pd.concat([train_creative, test_creative])\r\n",
        "  \r\n",
        "    train_creative[neighboring_indices] = 0\r\n",
        "    test_creative[neighboring_indices] = 0\r\n",
        "\r\n",
        "    for index, row in train_creative.iterrows():\r\n",
        "      priority_sum = 0\r\n",
        "      covered = {row[\"FIPS\"]}\r\n",
        "      neighbors = set()\r\n",
        "      for i in range(2):\r\n",
        "        for fips in covered:\r\n",
        "          neighbors.update(get_neighbors(fips))\r\n",
        "        neighbors = neighbors.difference(covered)\r\n",
        "        for neighbor in neighbors:\r\n",
        "          query = combined_creative[combined_creative[\"FIPS\"] == neighbor]\r\n",
        "          try:\r\n",
        "            res = query.iloc[0]\r\n",
        "            weight = 1 / (i + 1)\r\n",
        "            row[\"*MedianIncome\"] += weight * res[\"MedianIncome\"]\r\n",
        "            row[\"*MigraRate\"] += weight *  res[\"MigraRate\"]\r\n",
        "            row[\"*BirthRate\"] += weight * res[\"BirthRate\"]\r\n",
        "            row[\"*DeathRate\"] += weight * res[\"DeathRate\"]\r\n",
        "            row[\"*BachelorRate\"] += weight * res[\"BachelorRate\"]\r\n",
        "            row[\"*UnemploymentRate\"] += weight * res[\"UnemploymentRate\"]\r\n",
        "            priority_sum += weight\r\n",
        "          except Exception:\r\n",
        "            pass\r\n",
        "        covered.update(neighbors)\r\n",
        "        neighbors.clear()\r\n",
        "      if (priority_sum > 0):\r\n",
        "        row[neighboring_indices] /= priority_sum\r\n",
        "      train_creative.loc[index, neighboring_indices] = row[neighboring_indices]\r\n",
        "\r\n",
        "    for index, row in test_creative.iterrows():\r\n",
        "      priority_sum = 0\r\n",
        "      covered = {row[\"FIPS\"]}\r\n",
        "      neighbors = set()\r\n",
        "      for i in range(2):\r\n",
        "        for fips in covered:\r\n",
        "          neighbors.update(get_neighbors(fips))\r\n",
        "        neighbors = neighbors.difference(covered)\r\n",
        "        for neighbor in neighbors:\r\n",
        "          query = combined_creative[combined_creative[\"FIPS\"] == neighbor]\r\n",
        "          try:\r\n",
        "            res = query.iloc[0]\r\n",
        "            weight = 1 / (i + 1)\r\n",
        "            row[\"*MedianIncome\"] += weight * res[\"MedianIncome\"]\r\n",
        "            row[\"*MigraRate\"] += weight *  res[\"MigraRate\"]\r\n",
        "            row[\"*BirthRate\"] += weight * res[\"BirthRate\"]\r\n",
        "            row[\"*DeathRate\"] += weight * res[\"DeathRate\"]\r\n",
        "            row[\"*BachelorRate\"] += weight * res[\"BachelorRate\"]\r\n",
        "            row[\"*UnemploymentRate\"] += weight * res[\"UnemploymentRate\"]\r\n",
        "            priority_sum += weight\r\n",
        "          except Exception:\r\n",
        "            pass\r\n",
        "        covered.update(neighbors)\r\n",
        "        neighbors.clear()\r\n",
        "      if (priority_sum > 0):\r\n",
        "        row[neighboring_indices] /= priority_sum\r\n",
        "      test_creative.loc[index, neighboring_indices] = row[neighboring_indices]\r\n",
        "\r\n",
        "    X_creative = train_creative[indices + neighboring_indices].to_numpy()\r\n",
        "    xTeCreative = test_creative[indices + neighboring_indices].to_numpy()\r\n",
        "\r\n",
        "    Y = get_labels(train_creative)\r\n",
        "\r\n",
        "    return X_creative, xTeCreative, Y\r\n",
        "\r\n",
        "X_neighborsaverage, xTe_neighborsaverage, Y = generateTestAndTrainNeighborsAverage(train_df, test_df)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bmVO3bfqcd1",
        "outputId": "3b789854-8e0f-40d7-ac1d-eb88244aa596"
      },
      "source": [
        "'''\r\n",
        "Our model functions (knn, svc, and nn) run a K-Fold validation on the training\r\n",
        "set and returns the model that was trained on the split that produce the highest\r\n",
        "accuracies. Here, we run the models on different datasets which each have a \r\n",
        "different set of features. We also print out the accuracies of each of these \r\n",
        "models and choose the model with the highest accuracy. The accuracy of each \r\n",
        "model is commented on the side.\r\n",
        "\r\n",
        "''' \r\n",
        "\r\n",
        "print(\"------------------2012---------------------\")\r\n",
        "knn_2012 = knn(X_2012, Y) #0.7661498708010337\r\n",
        "svc_2012 = svc(X_2012, Y) #0.8369565217391306\r\n",
        "nn_2012 = neural_network(X_2012, Y) #0.7799310938845824\r\n",
        "\r\n",
        "print(\"----------------NEIGHBORS-------------------\")\r\n",
        "knn_neighbors = knn(X_neighbors, Y) #0.7713333333333333\r\n",
        "svc_neighbors = svc(X_neighbors, Y) #0.8647342995169083\r\n",
        "nn_neighbors = neural_network(X_neighbors, Y) #0.8115384615384615\r\n",
        "\r\n",
        "print(\"---------------NEIGHBORS AVERAGE-------------\")\r\n",
        "knn_neighborsaverage = knn(X_neighborsaverage, Y) #0.7297619047619048\r\n",
        "svc_neighborsaverage = svc(X_neighborsaverage, Y) #0.8937198067632851\r\n",
        "nn_neighborsaverage = neural_network(X_neighborsaverage, Y) #0.7940476190476191\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------2012---------------------\n",
            "0.7661498708010337\n",
            "0.8369565217391306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7799310938845824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------NEIGHBORS-------------------\n",
            "0.7713333333333333\n",
            "0.8647342995169083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8115384615384615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------NEIGHBORS AVERAGE-------------\n",
            "0.7297619047619048\n",
            "0.8937198067632851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7940476190476191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1sT3T30JcZ7"
      },
      "source": [
        "<h3>3.2 Final Results:</h3><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8uZDhPSJcZ7"
      },
      "source": [
        "Our basic submission (which was running the k nearest neighbor model looking at 5 neighbors with the basic feature set) gave an accuracy of 0.70361. Our creative model (which was running an svm model with a weighted accuracy and a feature set that took into account the neighboring counties features) gave an accuracy of 0.77655, so our model improved by about 7%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avSPg22XJcZ7"
      },
      "source": [
        "For our final creative model, we used an kernelized SVM with 10-fold validation and weighted class-labels to train the training dataset generated by `generateTestAndTrainNeighborsAverage`, which duplicates every instance variable in the basic dataset, and sets those variables equal to the weighted average of the values of those instance variables for each county's neighbors. This extra feature was added to take into account the features of counties in proximity to the county we are prediction. We wanted to add this extra feature since we had a hypothesis that counties are likely to vote in a similar manner to their neighbors. \r\n",
        "\r\n",
        "We also wanted to use weighted class-labels because when training the data set, we wanted to penalize incorrect predictions of Democratic counties over Republican ones because there are alot more Republican counties in the training set.  \r\n",
        "\r\n",
        "Another feature we tried out, but had an insignificant effect on improving our prediction accuracy, was looking at data from 2012 and seeing whether the changes in certain features over the years had an effect on which counties were more likely to vote Democtatic or Republican. When looking at the training data, we observed certain correlations to a county's change in median income/unemplyoment rate with a change in party affiliation. To encompass this feature in our training set, we incorporated several additional columns that represented the change in median income and unemployment from 2012 to 2016. \r\n",
        "\r\n",
        "Another feature we attempted to leverage in training our data was observing the number of direct neighbors a particular county had. We hypothesized that this feature might have an impact since costal counties tend to vote Democratic and also have less neighbors. However, encoding this feature had a minimal effect on our accuracies so we ended up pursuing other approaches of feature extraction.\r\n",
        "\r\n",
        "We also tried using a neural network model with `sklearn` that gave promising results. But, during the backpropagation portion of the algorithm, we wanted to used a weighted class-labels method to update the weights, but the `sklearn` package for the neural network model did not give us that option. Therefore, we instead implemented the weighted class-labels method in the svm model which ultimately gave better results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-_BqzOJcZ7"
      },
      "source": [
        "<h2>Part 4: Model Output and Post-Processing</h2><p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNeiRTaUJcZ7"
      },
      "source": [
        "# This function runs a model on our training dataset and outputs a \n",
        "# csv file with the predicted labels.\n",
        "\n",
        "def create_csv(model, fileName, xTe):\n",
        "  '''\n",
        "  Parameters: model is the trained model we want to\n",
        "  run on the test set, fileName is the chosen fileName \n",
        "  to write the predictions, xTe is the preprocessed \n",
        "  test set\n",
        "  Returns: Function returns a csv file with the test\n",
        "  set predictions\n",
        "  '''\n",
        "  data = test_df['FIPS']\n",
        "  df = pd.DataFrame(data , columns = ['FIPS']) \n",
        "  df[\"Result\"] = model.predict(xTe)\n",
        "  df.to_csv (fileName, index = False, header=True)\n",
        "\n",
        "create_csv(svc_neighborsaverage, \"svc_creative.csv\", xTe_neighborsaverage)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PxQ3-8mJcZ7"
      },
      "source": [
        "<h2>Part 5: References</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weiPX7nSJcZ7"
      },
      "source": [
        "\r\n",
        "\r\n",
        "* Documentation On Neural Networks: https://scikit-learn.org/stable/modules/neural_networks_supervised.html\r\n",
        "\r\n",
        "*  Documentation on Neural Networks: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\r\n",
        "* Documentation on K Nearest Neighbors: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\r\n",
        "* Documentation on skLearn: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\r\n",
        "\r\n"
      ]
    }
  ]
}